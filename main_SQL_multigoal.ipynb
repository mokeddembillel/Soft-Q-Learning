{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mokeddembillel/Soft-Q-Learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Soft-Q-Learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\a\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "C:\\Users\\a\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6e929491efe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mepisode_length\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Source Code\\GIT repositories\\Data science\\Soft-Q-Learning\\SQL_torch.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, steps)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;31m# kappa grad_kappa= T.empty(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;31m# for i in range(self.batch_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_kappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrbf_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_svgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_svgd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (bs, np, ad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Source Code\\GIT repositories\\Data science\\Soft-Q-Learning\\SQL_torch.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(self, input_1, input_2, h_min)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# Get median.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mmedian_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_sq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mmedian_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmedian_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m# N * 1 * k_upd * 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pybulletgym\n",
    "from SQL_torch import Agent\n",
    "from utils import plot_learning_curve\n",
    "from gym import wrappers\n",
    "import math\n",
    "from multigoal import MultiGoalEnv\n",
    "import torch as T\n",
    "from plotter import QFPolicyPlotter\n",
    "from networks import SamplingNetwork\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env= MultiGoalEnv()\n",
    "    hid = 256\n",
    "    l = 2\n",
    "    \n",
    "    agent = Agent(lambda : gym.make(env), \n",
    "                  hidden_dim=[256, 256], replay_size=int(1e6), pi_lr=1e-3, \n",
    "                  q_lr=1e-3, batch_size=500, noise_dim=2, n_particles=16, gamma=0.99)\n",
    "    \n",
    "    epochs=30\n",
    "    update_every=1\n",
    "    update_after=2000\n",
    "    max_ep_len=30\n",
    "    start_steps=0\n",
    "    steps_per_epoch=400\n",
    "    # Prepare for interaction with environment\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    \n",
    "    \n",
    "    \n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    \n",
    "    for t in range(total_steps):\n",
    "        \n",
    "        # Until start_steps have elapsed, randomly sample actions\n",
    "        # from a uniform distribution for better exploration. Afterwards, \n",
    "        # use the learned policy (with some noise, via act_noise). \n",
    "        if t > start_steps:\n",
    "            a = agent.get_sample(o)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        #print(\"t=\",t,\"ep_ret=\",ep_ret, \"ep_len=\",ep_len)\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        agent.replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update \n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "\n",
    "        # End of trajectory handling\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            #logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        \n",
    "        # Update handling\n",
    "        if t >= update_after and t % update_every == 0:\n",
    "            batch = agent.replay_buffer.sample_batch(agent.batch_size)\n",
    "            #print(\"before updating..\")\n",
    "            agent.learn(data=batch)\n",
    "            #print(\"after updating..\")\n",
    "\n",
    "        # End of epoch handling\n",
    "        if (t+1) % steps_per_epoch == 0:\n",
    "            epoch = (t+1) // steps_per_epoch\n",
    "            print(\"epoch=\",epoch)\n",
    "            agent.plot_paths(epoch)\n",
    "\n",
    "            # Save model\n",
    "            #if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "                #logger.save_state({'env': env}, None)\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            # agent.test_agent()\n",
    "    plotter = QFPolicyPlotter(qf = agent.Q_Network, ss=agent.SVGD_Network, obs_lst=[[0,0],[-2.5,-2.5],[2.5,2.5],[-2.5,2.5],[2.5,-2.5]], default_action =[np.nan,np.nan], n_samples=100)\n",
    "    plotter.draw()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-poison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
