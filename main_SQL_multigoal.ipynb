{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\a\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "C:\\Users\\a\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6e929491efe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mepisode_length\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Source Code\\GIT repositories\\Data science\\Soft-Q-Learning\\SQL_torch.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, steps)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;31m# kappa grad_kappa= T.empty(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;31m# for i in range(self.batch_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_kappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrbf_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_svgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_svgd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (bs, np, ad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Source Code\\GIT repositories\\Data science\\Soft-Q-Learning\\SQL_torch.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(self, input_1, input_2, h_min)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# Get median.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mmedian_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_sq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mmedian_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmedian_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m# N * 1 * k_upd * 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pybullet_envs\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybulletgym\n",
    "from SQL_torch import Agent\n",
    "from utils import plot_learning_curve\n",
    "from gym import wrappers\n",
    "import math\n",
    "from multigoal import MultiGoalEnv\n",
    "import torch as T\n",
    "from plotter import QFPolicyPlotter\n",
    "\n",
    "def plot(agent):\n",
    "    paths = []\n",
    "    actions_plot=[]\n",
    "    env = MultiGoalEnv()\n",
    "    n_games = 50\n",
    "    max_episode_length = 20\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        score = 0\n",
    "        path = {'infos':{'pos':[]}}\n",
    "        while not done:\n",
    "            env.render()\n",
    "            #print('state: ', np.squeeze(observation))\n",
    "            action = agent.get_action_svgd(T.from_numpy(observation).unsqueeze(0).double()).squeeze().detach().numpy()\n",
    "\n",
    "            #print('ac: ', action[0].cpu().detach().numpy())\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            path['infos']['pos'].append(observation)\n",
    "            \n",
    "            if episode_length == max_episode_length:\n",
    "                done = True\n",
    "            episode_length += 1\n",
    "            \n",
    "            #print('re:', reward)\n",
    "            score += reward\n",
    "            observation = observation_\n",
    "        paths.append(path)\n",
    "        \n",
    "        score = score / 200\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-20:])\n",
    "        \n",
    "    env.render_rollouts(paths, fout=\"test_%d.png\" % i)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = MultiGoalEnv()\n",
    "    # print(env.observation_space.shape)\n",
    "    # print(env.action_space.shape)\n",
    "    agent = Agent(state_dim=env.observation_space.shape[0], env=env,\n",
    "            action_dim=env.action_space.shape[0], n_particles=32, batch_size=64, reward_scale=0.1, max_action=env.action_space.high)\n",
    "    n_games = 50\n",
    "    # uncomment this line and do a mkdir tmp && mkdir video if you want to\n",
    "    # record video of the agent playing the game.\n",
    "    #env = wrappers.Monitor(env, 'tmp/video', video_callable=lambda episode_id: True, force=True)\n",
    "    filename = 'inverted_pendulum.png'\n",
    "    figure_file = 'plots/' + filename\n",
    "\n",
    "    #print(env.action_space.high)\n",
    "    \n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "    load_checkpoint = False\n",
    "    \n",
    "    max_episode_length = 100\n",
    "\n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "        env.render(mode='human')\n",
    "\n",
    "    for i in range(n_games):\n",
    "        \n",
    "        #observation = env.reset(init_state=[2.5, 2.5])\n",
    "        observation = env.reset()\n",
    "        episode_length = 0\n",
    "        \n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        while not done:\n",
    "            #env.render()\n",
    "            #print('state: ', np.squeeze(observation))\n",
    "            action = agent.get_action_svgd(T.from_numpy(observation).unsqueeze(0)).squeeze().detach().numpy()\n",
    "            \n",
    "           \n",
    "            #print('ac: ', np.squeeze(action))\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            \n",
    "            if episode_length == max_episode_length:\n",
    "                done = True\n",
    "            #print(episode_length)\n",
    "            #print('re:', reward)\n",
    "            #print('Q: ', np.squeeze(env.get_Q()))\n",
    "            score += reward\n",
    "            agent.remember(observation, action, reward, observation_, done)\n",
    "            if not load_checkpoint:\n",
    "                agent.learn(episode_length)\n",
    "            observation = observation_\n",
    "            episode_length += 1\n",
    "\n",
    "            \n",
    "        score = score \n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-20:])\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            # if not load_checkpoint:\n",
    "            #     agent.save_models()\n",
    "        print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "        \n",
    "        plot(agent)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #agent.actor.sample_normal(T.FloatTensor([2.5,2.5]).repeat([100,1]))[0].detach().numpy()\n",
    "    \n",
    "    plotter = QFPolicyPlotter(qf = agent.Q_Network, agent=agent, obs_lst=[[-2,0],[0,2],[2.5,2.5]], default_action =[np.nan,np.nan], n_samples=50)\n",
    "    plotter.draw()\n",
    "\n",
    "\n",
    "    env.close()  \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-poison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
